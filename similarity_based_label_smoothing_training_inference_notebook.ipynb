{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0tmyUgKFwGu"
   },
   "source": [
    "# Install and Load specific libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGNRyHsXFQlI"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!git clone https://github.com/google-research/bleurt.git\n",
    "!pip install bert_score\n",
    "!pip install datasets\n",
    "!pip install sacrebleu\n",
    "!pip install -q transformers\n",
    "!pip install --upgrade nltk\n",
    "!pip install rouge_score\n",
    "\n",
    "import os\n",
    "os.chdir('bleurt')\n",
    "\n",
    "!pip install .\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1wZUSnEZpuC"
   },
   "outputs": [],
   "source": [
    "\"\"\"The training, testing and validation data can be downloaded from here: \n",
    "    https://drive.google.com/drive/folders/18VKDa4cB8gW8pMypARc6pyBWGFFUIPKw?usp=sharing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5H5DIybyF8yc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from datasets import load_metric\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "import Models\n",
    "import shutil\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "tf.compat.v1.flags.DEFINE_string('f','','')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMByLTF-GrwO"
   },
   "outputs": [],
   "source": [
    "bertscore = load_metric(\"bertscore\")\n",
    "bleu = load_metric(\"sacrebleu\")\n",
    "meteor = load_metric(\"meteor\")\n",
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r0Cnb4_FaY6"
   },
   "source": [
    "# Run Experiments: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jH-3o5eGPB_"
   },
   "outputs": [],
   "source": [
    "#Load the experiment dict\n",
    "experiments_df = pd.read_csv('./experiment_parameters.csv')\n",
    "PATH = \"<path>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzxJisjIFdN7"
   },
   "outputs": [],
   "source": [
    "\"\"\" This loop will run all the 60 experiments seqentially, and save the best model\n",
    "    and training + validation metrics. It is recommended to break down the \n",
    "    experiments_df into sets of expertiments, and run the sets independently in multiple GPUs\n",
    "    to reduce overall runtime.\n",
    "\"\"\"\n",
    "\n",
    "for ix, row in experiments_df.iterrows():\n",
    "  name = row['name']\n",
    "  loss = row['loss']\n",
    "  wordnet = row['wordnet']\n",
    "  similarity = row['similarity']\n",
    "  dataset = row['dataset']\n",
    "  smoothen = row['smoothen']\n",
    "\n",
    "  \"\"\" Remove a handful of examples from each dataset which have errors in them. In total, \n",
    "      we remove 2 examples from ED dataset's validation set,  9 examples from DD train set,\n",
    "      and 1 from DD test set\"\"\"\n",
    "\n",
    "  if dataset == 'empatheticdialogues':\n",
    "    experiment_dict = pickle.load(open('./experiment_dict_v3.pkl', 'rb'))\n",
    "    word_similarity_labels = pickle.load(open('./word_similarity_labels_v2.pkl', 'rb'))\n",
    "    experiment_dict['valid_dict']['X_chat'] = [i for ix, i in enumerate(experiment_dict['valid_dict']['X_chat']) if ix != 4690]\n",
    "    experiment_dict['valid_dict']['Y_chat'] = [i for ix, i in enumerate(experiment_dict['valid_dict']['Y_chat']) if ix != 4690]\n",
    "    \n",
    "  else:\n",
    "    experiment_dict = pickle.load(open('./experiment_dict_dailydialogue.pkl', 'rb'))\n",
    "    word_similarity_labels = pickle.load(open('./word_similarity_labels_dailydialogue.pkl', 'rb'))\n",
    "    train_ = [4433, 9123, 9124, 10075, 13853, 18344, 22919, 27316, 28067]\n",
    "    test_ = [123]\n",
    "    experiment_dict['train_dict']['X_chat'] = [i for ix, i in enumerate(experiment_dict['train_dict']['X_chat']) if ix not in train_]\n",
    "    experiment_dict['train_dict']['Y_chat'] = [i for ix, i in enumerate(experiment_dict['train_dict']['Y_chat']) if ix not in train_]\n",
    "    experiment_dict['test_dict']['X_chat'] = [i for ix, i in enumerate(experiment_dict['test_dict']['X_chat']) if ix not in test_]\n",
    "    experiment_dict['test_dict']['Y_chat'] = [i for ix, i in enumerate(experiment_dict['test_dict']['Y_chat']) if ix not in test_]\n",
    "\n",
    "  print(experiment_dict.keys())\n",
    "  print(len(experiment_dict['vocab_word2vec']))\n",
    "\n",
    "  \"\"\" Create the config file for the current experiment \"\"\"\n",
    "  config = {'name':name,\n",
    "            'wordnet':wordnet,\n",
    "            'dataset':dataset,\n",
    "            'tgt_sos':0,\n",
    "            'src_pad_idx':1,\n",
    "            'tgt_pad_idx':1,\n",
    "            'device':torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "            'loss':loss,\n",
    "            'smoothing': smoothen,\n",
    "            'hid_dim':300,\n",
    "            'out_dim':len(experiment_dict['word2index']),\n",
    "            'pretrained_encoder':'none',\n",
    "            'encoder_trainable':True,\n",
    "            'max_len':50,\n",
    "            'max_tgt_len':150,\n",
    "            'n_layers':3,\n",
    "            'n_heads':10,\n",
    "            'pf_dim':512,\n",
    "            'dropout':0.1,\n",
    "            'batch_size':64,\n",
    "            'clip':1.0,\n",
    "            'optimizer':'AdamW',\n",
    "            'lr':2e-4,\n",
    "            'embedding_pretrained':True,\n",
    "            'epochs':15,\n",
    "            'sample_index':0,\n",
    "            'cosine_threshold':similarity,\n",
    "            'cosine_power':7,\n",
    "            'weights_matrix':utils.get_word_vectors(experiment_dict['vocab_word2vec'])\n",
    "            }\n",
    "  loss_settings = utils.get_loss_settings(loss)\n",
    "  config = {**config, **loss_settings}\n",
    "  print(config)\n",
    "\n",
    "  if config['wordnet'] == 0:\n",
    "    experiment_dict['similarity'] = utils.format_similarity_matrix_smoothen(experiment_dict['similarity'], \n",
    "                                                                            config['cosine_threshold'],\n",
    "                                                                            config['smoothing'])\n",
    "  else:\n",
    "    experiment_dict['similarity'] = utils.format_similarity_matrix_wordnet_smoothen(experiment_dict['similarity'], \n",
    "                                                                                    word_similarity_labels,\n",
    "                                                                                    config['cosine_threshold'],\n",
    "                                                                                    config['smoothing'])\n",
    "  model, criterion, optimizer = utils.init_all(config)\n",
    "  tot_t_loss, tot_v_loss = [], []\n",
    "  bert_score_list, bleu_score_list, bleurt_score_list, bert_score_hash_list = [], [], [], []\n",
    "\n",
    "  best_valid_loss = float('inf')\n",
    "\n",
    "  \"\"\" Start training and validation of the model for the current experiment.\n",
    "      All the losses and metrics are logged. \"\"\"\n",
    "  for epoch in range(1, config['epochs']+1):\n",
    "    train_iterator, valid_iterator, test_iterator = utils.get_iterators(experiment_dict, config)\n",
    "    start_time = time.time()\n",
    "    tr_l = utils.train(model, train_iterator, optimizer, criterion, config['clip'])\n",
    "    tot_t_loss.append(tr_l)\n",
    "    tr_v, hypothesis, corpus = utils.evaluate(model, valid_iterator, criterion, \n",
    "                                              experiment_dict['index2word'], \n",
    "                                              config, test=False)\n",
    "    tot_v_loss.append(tr_v)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = utils.epoch_time(start_time, end_time)\n",
    "\n",
    "    bleu_score = bleu.compute(predictions=hypothesis, references=[[i] for i in corpus])\n",
    "    bert_score = bertscore.compute(predictions=hypothesis, references=corpus, lang='en')\n",
    "\n",
    "    bert_score_list.append(np.mean(bert_score['f1'].tolist()))\n",
    "    bleu_score_list.append(bleu_score['score'])\n",
    "    bert_score_hash_list.append(bert_score['hashcode'])\n",
    "\n",
    "    print(\"\\nMETRIC scores : \")\n",
    "    print(\"BLEU: \\n\", bleu_score_list[-1])\n",
    "    print(\"BERTScore: \\n\", bert_score_list[-1])\n",
    "    print(\"Few hypothesis:\\n\",hypothesis[:4])\n",
    "    print(\"Few corpus:\\n\",corpus[:4])\n",
    "    print(\"\\n\")\n",
    "    model_name = name+\".pt\"\n",
    "    if tr_v < best_valid_loss:\n",
    "        best_valid_loss = tr_v\n",
    "        torch.save(model.state_dict(), \"Best-\"+model_name)\n",
    "        print(\"\\nBest Model Saved !!\")\n",
    "    print(\"\\n\")\n",
    "    print(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Total Loss: {tr_l:.3f}')\n",
    "    print(f'\\tVal. Total Loss: {tr_v:.3f}')\n",
    "    print(\"_________________________________________________________________\")\n",
    "\n",
    "  results_dict = utils.make_results_dict(tot_t_loss, tot_v_loss, bert_score_list, bleu_score_list, bleurt_score_list, bert_score_hash_list, best_valid_loss, model_name, config)\n",
    "  results_name = model_name+'_results.pkl'\n",
    "  best_model_name = \"Best-\"+model_name\n",
    "  pickle.dump(results_dict, open('./'+results_name, 'wb'))\n",
    "  shutil.copy2('./'+results_name, PATH+'/results/')\n",
    "  shutil.copy2('./'+best_model_name, PATH+'/models/')\n",
    "  print(\"EXPERIMENT DONE !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7Zq9dknFeVF"
   },
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAigUpZAImUZ"
   },
   "outputs": [],
   "source": [
    "results_loc = PATH +'/results/'\n",
    "models_loc = PATH + '/models/'\n",
    "experiments_loc = PATH + '/experiment_parameters.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTmp_u_1IicE"
   },
   "outputs": [],
   "source": [
    "result_filenames = os.listdir(results_loc)\n",
    "result_filenames = [fn for fn in result_filenames if fn.replace('.pt_results.pkl', '') in all_valid_names]\n",
    "\n",
    "model_filenames = os.listdir(models_loc)\n",
    "model_filenames = [fn for fn in model_filenames if fn.replace('.pt', '').replace('Best-', '') in all_valid_names]\n",
    "\n",
    "len(result_filenames), len(model_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JK3GPqbFIeZJ"
   },
   "outputs": [],
   "source": [
    "name_to_model_map = {}\n",
    "for filename in result_filenames:\n",
    "    tmp = pickle.load(open(results_loc+filename, 'rb'))\n",
    "    k = tmp['config']['name']\n",
    "    v = {'tot_t_loss': tmp['tot_t_loss'],\n",
    "         'tot_v_loss': tmp['tot_v_loss']}\n",
    "    name_to_model_map[k] = {'model_name': tmp['model_name'],\n",
    "                            'config': tmp['config']}\n",
    "    if tmp['config']['dataset'] == 'dailydialog':\n",
    "        dailydialog_results[k] = v\n",
    "        dailydialog_best_val.append([k.replace('-dt-01_03_2021', '').replace('ds-dailydialog-', ''), max(tmp['bleu_score_list'])])\n",
    "    else:\n",
    "        empatheticdialogues_results[k] = v\n",
    "        empatheticdialogues_best_val.append([k.replace('-dt-01_03_2021', '').replace('ds-empatheticdialogues-', ''), max(tmp['bleu_score_list'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QqKMQ7wFfpa"
   },
   "outputs": [],
   "source": [
    "\"\"\" For each experiment, this loop will load the best Pytorch model, run the model on the \n",
    "    test set, and compute the automatic evaluation metrics \"\"\"\n",
    "    \n",
    "test_dict_results = {}\n",
    "for k,v in name_to_model_map.items():\n",
    "    print(\"Processing for \",k)\n",
    "    config = v['config']\n",
    "    model_name = v['model_name']\n",
    "    if config['dataset'] == 'empatheticdialogues':\n",
    "        experiment_dict = pickle.load(open('./experiment_dict_v3.pkl', 'rb'))\n",
    "        word_similarity_labels = pickle.load(open('./word_similarity_labels_v2.pkl', 'rb'))\n",
    "        experiment_dict['valid_dict']['X_chat'] = [i for ix, i in enumerate(experiment_dict['valid_dict']['X_chat']) if ix != 4690]\n",
    "        experiment_dict['valid_dict']['Y_chat'] = [i for ix, i in enumerate(experiment_dict['valid_dict']['Y_chat']) if ix != 4690]\n",
    "    else:\n",
    "        experiment_dict = pickle.load(open('./experiment_dict_dailydialogue.pkl', 'rb'))\n",
    "        word_similarity_labels = pickle.load(open('./word_similarity_labels_dailydialogue.pkl', 'rb'))\n",
    "        train_ = [4433, 9123, 9124, 10075, 13853, 18344, 22919, 27316, 28067]\n",
    "        test_ = [123]\n",
    "        experiment_dict['train_dict']['X_chat'] = [i for ix, i in enumerate(experiment_dict['train_dict']['X_chat']) if ix not in train_]\n",
    "        experiment_dict['train_dict']['Y_chat'] = [i for ix, i in enumerate(experiment_dict['train_dict']['Y_chat']) if ix not in train_]\n",
    "        experiment_dict['test_dict']['X_chat'] = [i for ix, i in enumerate(experiment_dict['test_dict']['X_chat']) if ix not in test_]\n",
    "        experiment_dict['test_dict']['Y_chat'] = [i for ix, i in enumerate(experiment_dict['test_dict']['Y_chat']) if ix not in test_]\n",
    "\n",
    "    config['weights_matrix'] = utils.get_word_vectors(experiment_dict['vocab_word2vec'])\n",
    "\n",
    "    test_iterator = utils.get_batch_data(experiment_dict, 'test_dict', config, plus=0)\n",
    "\n",
    "    model, criterion, optimizer = utils.init_all(config)\n",
    "    model.load_state_dict(torch.load(models_loc+model_name))\n",
    "    model.eval()\n",
    "    tr_v, hypothesis, corpus = utils.evaluate(model, test_iterator, criterion, \n",
    "                                              experiment_dict['index2word'], \n",
    "                                              config, test=True)\n",
    "    bleu_score = bleu.compute(predictions=hypothesis, references=[[i] for i in corpus])\n",
    "    bert_score = bertscore.compute(predictions=hypothesis, references=corpus, lang='en')\n",
    "    mime_bleu_score = utils.get_BLEU_score(hypothesis, [[i] for i in corpus])\n",
    "    test_dict_results[k] = {'bleu_score':bleu_score,\n",
    "                            'bert_score':bert_score,\n",
    "                            'mime_bleu_score':mime_bleu_score,\n",
    "                            'hypothesis':hypothesis,\n",
    "                            'corpus':corpus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnwXKbqrJDit"
   },
   "outputs": [],
   "source": [
    "\"\"\" This loop will compute the Rouge 1, 2, L and the Meteor score for each experiment\"\"\"\n",
    "\n",
    "test_dict_results_enriched = {}\n",
    "for k,v in test_dict_results.items():\n",
    "  # print(k)\n",
    "  rouge_score = rouge.compute(predictions=v['hypothesis'], references=v['corpus'], \n",
    "                              rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "  meteor_score = meteor.compute(predictions=v['hypothesis'], references=v['corpus'])\n",
    "\n",
    "  test_dict_results_enriched[k] = v\n",
    "  test_dict_results_enriched[k]['rouge1'] = rouge_score['rouge1'].mid.fmeasure\n",
    "  test_dict_results_enriched[k]['rouge2'] = rouge_score['rouge2'].mid.fmeasure\n",
    "  test_dict_results_enriched[k]['rougeL'] = rouge_score['rougeL'].mid.fmeasure\n",
    "  test_dict_results_enriched[k]['meteor'] = meteor_score['meteor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQqo0gDMJDs1"
   },
   "outputs": [],
   "source": [
    "\"\"\" This loop consolidates all the calculated metrics for each experiment in a \n",
    "    Pandas Data Frame, and saves it as a csv file\"\"\"\n",
    "test_results_lst = []\n",
    "for k,v in test_dict_results_enriched.items():\n",
    "  sacrebleu = v['bleu_score']['score']\n",
    "  bleu = v['mime_bleu_score']\n",
    "  bleurt = v['bleurt_score']\n",
    "  bert_score = v['bert_score']['f1'].mean().item()\n",
    "  _,ds_name,_,loss,_,smoothing,_,similarity,_,word_net,_,_ = k.split('-')\n",
    "  test_results_lst.append([ds_name, loss, smoothing, similarity, word_net, \n",
    "                          sacrebleu, bleu, bleurt, bert_score, v['rouge1'], v['rouge2'],\n",
    "                          v['rougeL'], v['meteor']])\n",
    "\n",
    "enriched_test_results_df = pd.DataFrame(test_results_lst, columns = ['dataset', 'loss', 'smoothing', 'similarity', 'word_net',\n",
    "                                                               'sacrebleu', 'bleu', 'bleurt', 'bert_score', 'rouge1', \n",
    "                                                               'rouge2', 'rougeL', 'meteor'])\n",
    "\n",
    "enriched_test_results_df.to_csv('./enriched_test_results_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhTYyJcMJg-w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "similarity_based_label_smoothing_training_inference_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
